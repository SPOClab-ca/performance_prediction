{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worse Fine Tuning\n",
    "\n",
    "Try to make BERT / RoBERTa worse by doing some additional pre-training on Wikipedia shuffled sentences.\n",
    "\n",
    "Author: Bai Li  \n",
    "Based loosely off this tutorial: https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  AutoModelForMaskedLM,\n",
    "  DataCollatorForLanguageModeling,\n",
    "  Trainer,\n",
    "  TrainingArguments,\n",
    "  pipeline,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# The GPU to use for training\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model\n",
    "\n",
    "To compare against corrupted model, try a simple fill-mask task with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "  \"fill-mask\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8505654335021973,\n",
       "  'token': 875,\n",
       "  'token_str': ' wrote',\n",
       "  'sequence': 'I wrote a book about animals.'},\n",
       " {'score': 0.04043307527899742,\n",
       "  'token': 33,\n",
       "  'token_str': ' have',\n",
       "  'sequence': 'I have a book about animals.'},\n",
       " {'score': 0.029625510796904564,\n",
       "  'token': 3116,\n",
       "  'token_str': ' write',\n",
       "  'sequence': 'I write a book about animals.'},\n",
       " {'score': 0.01930156722664833,\n",
       "  'token': 1027,\n",
       "  'token_str': ' published',\n",
       "  'sequence': 'I published a book about animals.'},\n",
       " {'score': 0.012223951518535614,\n",
       "  'token': 222,\n",
       "  'token_str': ' did',\n",
       "  'sequence': 'I did a book about animals.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"I <mask> a book about animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct scrambled sentences from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /h/zining/.cache/huggingface/modules/datasets_modules/datasets/wikitext/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20 (last modified on Tue Aug 31 17:24:58 2021) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f22e7f179d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/h/zining/.conda/envs/transformers4/lib/python3.8/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    self.close()\n",
      "  File \"/h/zining/.conda/envs/transformers4/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Reusing dataset wikitext (/h/zining/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_dataset('wikitext', 'wikitext-2-v1', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "scrambled_sentences = []\n",
    "for sent in wiki_dataset:\n",
    "  sent_toks = sent['text'].split()\n",
    "  random.shuffle(sent_toks)\n",
    "  scrambled_sentences.append(' '.join(sent_toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffledWikiDataset(Dataset):\n",
    "  def __len__(self):\n",
    "    return len(scrambled_sentences)\n",
    "  def __getitem__(self, i):\n",
    "    return tokenizer(scrambled_sentences[i], max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do more pre-training to degrade model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# This controls the amount of degradation.\n",
    "corrupt_training_steps = 6400\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./checkpoints/',\n",
    "  per_device_train_batch_size=16,\n",
    "  max_steps=corrupt_training_steps,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    "  train_dataset=ShuffledWikiDataset(),\n",
    "  args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 36718\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mziningzhu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./checkpoints/</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziningzhu/huggingface\" target=\"_blank\">https://wandb.ai/ziningzhu/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziningzhu/huggingface/runs/3hlspb2g\" target=\"_blank\">https://wandb.ai/ziningzhu/huggingface/runs/3hlspb2g</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/ssd002/home/zining/probing_shortcuts/notebooks/wandb/run-20211222_183710-3hlspb2g</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6400' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6400/6400 56:45, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.658600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoints/checkpoint-500\n",
      "Configuration saved in ./checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1000\n",
      "Configuration saved in ./checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1500\n",
      "Configuration saved in ./checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-2000\n",
      "Configuration saved in ./checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-2500\n",
      "Configuration saved in ./checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-3000\n",
      "Configuration saved in ./checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-3500\n",
      "Configuration saved in ./checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-4000\n",
      "Configuration saved in ./checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-4500\n",
      "Configuration saved in ./checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-5000\n",
      "Configuration saved in ./checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-5500\n",
      "Configuration saved in ./checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-6000\n",
      "Configuration saved in ./checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-6000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6400, training_loss=4.858393764495849, metrics={'train_runtime': 3413.6661, 'train_samples_per_second': 29.997, 'train_steps_per_second': 1.875, 'total_flos': 6720978055895136.0, 'train_loss': 4.858393764495849, 'epoch': 2.79})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try fill-mask on corrupted model\n",
    "\n",
    "As we expected, the predictions are still reasonable, but worse (eg: top prediction is the same but confidence score is a lot lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "  \"fill-mask\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I wrote a book about animals.',\n",
       "  'score': 0.3081257939338684,\n",
       "  'token': 875,\n",
       "  'token_str': ' wrote'},\n",
       " {'sequence': 'I have a book about animals.',\n",
       "  'score': 0.08756621181964874,\n",
       "  'token': 33,\n",
       "  'token_str': ' have'},\n",
       " {'sequence': 'I published a book about animals.',\n",
       "  'score': 0.0630703717470169,\n",
       "  'token': 1027,\n",
       "  'token_str': ' published'},\n",
       " {'sequence': 'I had a book about animals.',\n",
       "  'score': 0.06285504996776581,\n",
       "  'token': 56,\n",
       "  'token_str': ' had'},\n",
       " {'sequence': 'I was a book about animals.',\n",
       "  'score': 0.05396825447678566,\n",
       "  'token': 21,\n",
       "  'token_str': ' was'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"I <mask> a book about animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to disk\n",
    "\n",
    "To load, do `AutoModelForMaskedLM.from_pretrained(model_path)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in checkpoints/roberta-base-corrupt-200-steps/config.json\n",
      "Model weights saved in checkpoints/roberta-base-corrupt-200-steps/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(f\"checkpoints/{model_name}-corrupt-{corrupt_training_steps}-steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
