{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict GLUE performances with probing\n",
    "Does probing results predict the GLUE task performances?  \n",
    "This notebook: merge GLUE classification results and probing results into a table, so I can do OLS analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "import numpy as np\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2275, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LM</th>\n",
       "      <th>layer</th>\n",
       "      <th>task</th>\n",
       "      <th>config</th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>rs</th>\n",
       "      <th>train_size_per_class</th>\n",
       "      <th>nclasses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0</td>\n",
       "      <td>bigram_shift</td>\n",
       "      <td>Full</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6560.2</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0</td>\n",
       "      <td>coordination_inversion</td>\n",
       "      <td>Full</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6560.2</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0</td>\n",
       "      <td>obj_number</td>\n",
       "      <td>Full</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6560.2</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0</td>\n",
       "      <td>odd_man_out</td>\n",
       "      <td>Full</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.693146</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693149</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693149</td>\n",
       "      <td>6560.2</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0</td>\n",
       "      <td>past_present</td>\n",
       "      <td>Full</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6560.2</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           LM  layer                    task config  \\\n",
       "0   embeddings_albert_base_v2      0            bigram_shift   Full   \n",
       "3   embeddings_albert_base_v2      0  coordination_inversion   Full   \n",
       "6   embeddings_albert_base_v2      0              obj_number   Full   \n",
       "9   embeddings_albert_base_v2      0             odd_man_out   Full   \n",
       "12  embeddings_albert_base_v2      0            past_present   Full   \n",
       "\n",
       "           model  train_acc  train_loss  val_acc  val_loss  test_acc  \\\n",
       "0   DecisionTree   0.500000    0.693147      0.5  0.693147       0.5   \n",
       "3   DecisionTree   0.500000    0.693147      0.5  0.693147       0.5   \n",
       "6   DecisionTree   0.500000    0.693147      0.5  0.693147       0.5   \n",
       "9   DecisionTree   0.500833    0.693146      0.5  0.693149       0.5   \n",
       "12  DecisionTree   0.500000    0.693147      0.5  0.693147       0.5   \n",
       "\n",
       "    test_loss      rs  train_size_per_class  nclasses  \n",
       "0    0.693147  6560.2                1200.0       2.0  \n",
       "3    0.693147  6560.2                1200.0       2.0  \n",
       "6    0.693147  6560.2                1200.0       2.0  \n",
       "9    0.693149  6560.2                1200.0       2.0  \n",
       "12   0.693147  6560.2                1200.0       2.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probing_df = pd.read_csv(\"../reports/probing_1200_per_class.csv\")\n",
    "probing_df = probing_df[probing_df[\"config\"]==\"Full\"]\n",
    "print(probing_df.shape)\n",
    "probing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>LM</th>\n",
       "      <th>init_lr</th>\n",
       "      <th>slurm_id</th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rte</td>\n",
       "      <td>embeddings_roberta_base</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5236891</td>\n",
       "      <td>0.7726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rte</td>\n",
       "      <td>embeddings_roberta_base_corr_500</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5215525</td>\n",
       "      <td>0.7148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rte</td>\n",
       "      <td>embeddings_roberta_base_corr_1000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5258357</td>\n",
       "      <td>0.7040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rte</td>\n",
       "      <td>embeddings_roberta_base_corr_2000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5258397</td>\n",
       "      <td>0.6859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rte</td>\n",
       "      <td>embeddings_roberta_base_corr_4000</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>5258437</td>\n",
       "      <td>0.5848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task                                 LM  init_lr  slurm_id  dev_acc\n",
       "0  rte            embeddings_roberta_base  0.00001   5236891   0.7726\n",
       "1  rte   embeddings_roberta_base_corr_500  0.00001   5215525   0.7148\n",
       "2  rte  embeddings_roberta_base_corr_1000  0.00001   5258357   0.7040\n",
       "3  rte  embeddings_roberta_base_corr_2000  0.00001   5258397   0.6859\n",
       "4  rte  embeddings_roberta_base_corr_4000  0.00001   5258437   0.5848"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_df = pd.read_csv(\"../reports/glue_classification_results.csv\")\n",
    "glue_df = glue_df[~glue_df.task.str.contains(\"processed\")]  # Only handle the raw fine-tuning classification results for now.\n",
    "print(glue_df.shape)\n",
    "glue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LM</th>\n",
       "      <th>init_lr</th>\n",
       "      <th>slurm_id</th>\n",
       "      <th>dev_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cola</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrpc</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qnli</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qqp</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rte</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LM  init_lr  slurm_id  dev_acc\n",
       "task                                \n",
       "cola  25       25        25       25\n",
       "mrpc  25       25        25       25\n",
       "qnli  25       25        25       25\n",
       "qqp   25       25        25       25\n",
       "rte   25       25        25       25\n",
       "sst2  25       25        25       25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_df.groupby(\"task\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LM</th>\n",
       "      <th>rte</th>\n",
       "      <th>cola</th>\n",
       "      <th>qnli</th>\n",
       "      <th>mrpc</th>\n",
       "      <th>sst2</th>\n",
       "      <th>qqp</th>\n",
       "      <th>bigram_shift_layer_0</th>\n",
       "      <th>coordination_inversion_layer_0</th>\n",
       "      <th>obj_number_layer_0</th>\n",
       "      <th>...</th>\n",
       "      <th>past_present_layer_11</th>\n",
       "      <th>subj_number_layer_11</th>\n",
       "      <th>tree_depth_layer_11</th>\n",
       "      <th>bigram_shift_layer_12</th>\n",
       "      <th>coordination_inversion_layer_12</th>\n",
       "      <th>obj_number_layer_12</th>\n",
       "      <th>odd_man_out_layer_12</th>\n",
       "      <th>past_present_layer_12</th>\n",
       "      <th>subj_number_layer_12</th>\n",
       "      <th>tree_depth_layer_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embeddings_roberta_base</td>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.8437</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.283714</td>\n",
       "      <td>0.850667</td>\n",
       "      <td>0.654333</td>\n",
       "      <td>0.787333</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>0.879667</td>\n",
       "      <td>0.815333</td>\n",
       "      <td>0.295238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embeddings_roberta_base_corr_500</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>0.8322</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>0.9164</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865333</td>\n",
       "      <td>0.704333</td>\n",
       "      <td>0.250381</td>\n",
       "      <td>0.685667</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.582667</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.294857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>embeddings_roberta_base_corr_1000</td>\n",
       "      <td>0.7040</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.259905</td>\n",
       "      <td>0.605667</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>0.297714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embeddings_roberta_base_corr_2000</td>\n",
       "      <td>0.6859</td>\n",
       "      <td>0.8341</td>\n",
       "      <td>0.9185</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.747667</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.539333</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.560667</td>\n",
       "      <td>0.870333</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.301143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embeddings_roberta_base_corr_4000</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.8399</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.785667</td>\n",
       "      <td>0.284190</td>\n",
       "      <td>0.559333</td>\n",
       "      <td>0.540667</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>0.865333</td>\n",
       "      <td>0.810333</td>\n",
       "      <td>0.302667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>embeddings_roberta_base_corr_6000</td>\n",
       "      <td>0.6679</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.8554</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867667</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.287619</td>\n",
       "      <td>0.552333</td>\n",
       "      <td>0.535667</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.571333</td>\n",
       "      <td>0.865333</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.297810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>embeddings_xlm_roberta_base</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.9068</td>\n",
       "      <td>0.8211</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>0.9093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757667</td>\n",
       "      <td>0.745667</td>\n",
       "      <td>0.267143</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.565333</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.558667</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>0.817333</td>\n",
       "      <td>0.309429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>embeddings_xlm_roberta_base_corr_500</td>\n",
       "      <td>0.5993</td>\n",
       "      <td>0.7881</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.260952</td>\n",
       "      <td>0.605333</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.284952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>embeddings_xlm_roberta_base_corr_1000</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.7939</td>\n",
       "      <td>0.9023</td>\n",
       "      <td>0.8284</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.783667</td>\n",
       "      <td>0.262762</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.517333</td>\n",
       "      <td>0.804667</td>\n",
       "      <td>0.525333</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>0.803667</td>\n",
       "      <td>0.293143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>embeddings_xlm_roberta_base_corr_2000</td>\n",
       "      <td>0.5884</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793667</td>\n",
       "      <td>0.777333</td>\n",
       "      <td>0.260857</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.495667</td>\n",
       "      <td>0.813000</td>\n",
       "      <td>0.507333</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.282571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>embeddings_xlm_roberta_base_corr_4000</td>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>0.8456</td>\n",
       "      <td>0.9278</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.775667</td>\n",
       "      <td>0.274381</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.509333</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.514333</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.295048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>embeddings_xlm_roberta_base_corr_6000</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810333</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.269143</td>\n",
       "      <td>0.504667</td>\n",
       "      <td>0.499667</td>\n",
       "      <td>0.807667</td>\n",
       "      <td>0.518667</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.810333</td>\n",
       "      <td>0.291143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>embeddings_albert_base_v2</td>\n",
       "      <td>0.7690</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.9163</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.799667</td>\n",
       "      <td>0.301905</td>\n",
       "      <td>0.821333</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.765667</td>\n",
       "      <td>0.577333</td>\n",
       "      <td>0.880667</td>\n",
       "      <td>0.794667</td>\n",
       "      <td>0.298190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>embeddings_albert_base_v2_corr_500</td>\n",
       "      <td>0.6173</td>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.9220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852667</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.295429</td>\n",
       "      <td>0.614667</td>\n",
       "      <td>0.546333</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.834333</td>\n",
       "      <td>0.813667</td>\n",
       "      <td>0.306762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>embeddings_albert_base_v2_corr_1000</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.8169</td>\n",
       "      <td>0.9114</td>\n",
       "      <td>0.8284</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856333</td>\n",
       "      <td>0.796333</td>\n",
       "      <td>0.287048</td>\n",
       "      <td>0.583667</td>\n",
       "      <td>0.548333</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.519667</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.798667</td>\n",
       "      <td>0.302762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>embeddings_albert_base_v2_corr_2000</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.7967</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.7581</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835667</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.299524</td>\n",
       "      <td>0.561333</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.515667</td>\n",
       "      <td>0.839333</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>embeddings_albert_base_v2_corr_4000</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>0.7996</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850333</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.298667</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.512333</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.309048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>embeddings_albert_base_v2_corr_6000</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>0.8025</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.7722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850333</td>\n",
       "      <td>0.801667</td>\n",
       "      <td>0.294952</td>\n",
       "      <td>0.550333</td>\n",
       "      <td>0.501333</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.526333</td>\n",
       "      <td>0.839667</td>\n",
       "      <td>0.794333</td>\n",
       "      <td>0.309429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>embeddings_microsoft_deberta_base</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>0.8946</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.804667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.773667</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.637667</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.869333</td>\n",
       "      <td>0.745667</td>\n",
       "      <td>0.262857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>embeddings_microsoft_deberta_base_corr_500</td>\n",
       "      <td>0.6787</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.293143</td>\n",
       "      <td>0.634333</td>\n",
       "      <td>0.571667</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.835667</td>\n",
       "      <td>0.672333</td>\n",
       "      <td>0.251524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>embeddings_microsoft_deberta_base_corr_1000</td>\n",
       "      <td>0.5740</td>\n",
       "      <td>0.8504</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8456</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862333</td>\n",
       "      <td>0.780667</td>\n",
       "      <td>0.291714</td>\n",
       "      <td>0.608667</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.260571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>embeddings_microsoft_deberta_base_corr_2000</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>0.8408</td>\n",
       "      <td>0.9224</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865667</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.294286</td>\n",
       "      <td>0.623333</td>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.600333</td>\n",
       "      <td>0.507667</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.721667</td>\n",
       "      <td>0.272762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>embeddings_microsoft_deberta_base_corr_4000</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.9220</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.7874</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.604667</td>\n",
       "      <td>0.566333</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.747667</td>\n",
       "      <td>0.282381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>embeddings_microsoft_deberta_base_corr_6000</td>\n",
       "      <td>0.6209</td>\n",
       "      <td>0.8341</td>\n",
       "      <td>0.9228</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862333</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.297810</td>\n",
       "      <td>0.610333</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.670333</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.859667</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.285810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>embeddings_xlnet_base_cased</td>\n",
       "      <td>0.7545</td>\n",
       "      <td>0.8035</td>\n",
       "      <td>0.9158</td>\n",
       "      <td>0.8676</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>0.7805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.287905</td>\n",
       "      <td>0.621667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.654667</td>\n",
       "      <td>0.542333</td>\n",
       "      <td>0.789667</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.274286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             LM     rte    cola    qnli  \\\n",
       "0                       embeddings_roberta_base  0.7726  0.8437  0.9251   \n",
       "1              embeddings_roberta_base_corr_500  0.7148  0.8322  0.9213   \n",
       "2             embeddings_roberta_base_corr_1000  0.7040  0.8380  0.9213   \n",
       "3             embeddings_roberta_base_corr_2000  0.6859  0.8341  0.9185   \n",
       "4             embeddings_roberta_base_corr_4000  0.5848  0.8399  0.9209   \n",
       "5             embeddings_roberta_base_corr_6000  0.6679  0.8293  0.9229   \n",
       "6                   embeddings_xlm_roberta_base  0.6534  0.7833  0.9068   \n",
       "7          embeddings_xlm_roberta_base_corr_500  0.5993  0.7881  0.8997   \n",
       "8         embeddings_xlm_roberta_base_corr_1000  0.5776  0.7939  0.9023   \n",
       "9         embeddings_xlm_roberta_base_corr_2000  0.5884  0.7919  0.8964   \n",
       "10        embeddings_xlm_roberta_base_corr_4000  0.6282  0.7929  0.8988   \n",
       "11        embeddings_xlm_roberta_base_corr_6000  0.5596  0.7804  0.8951   \n",
       "12                    embeddings_albert_base_v2  0.7690  0.8111  0.9163   \n",
       "13           embeddings_albert_base_v2_corr_500  0.6173  0.8102  0.9101   \n",
       "14          embeddings_albert_base_v2_corr_1000  0.5668  0.8169  0.9114   \n",
       "15          embeddings_albert_base_v2_corr_2000  0.6390  0.7967  0.9070   \n",
       "16          embeddings_albert_base_v2_corr_4000  0.6029  0.7996  0.9085   \n",
       "17          embeddings_albert_base_v2_corr_6000  0.6029  0.8025  0.9065   \n",
       "18            embeddings_microsoft_deberta_base  0.6643  0.8533  0.9323   \n",
       "19   embeddings_microsoft_deberta_base_corr_500  0.6787  0.8428  0.9257   \n",
       "20  embeddings_microsoft_deberta_base_corr_1000  0.5740  0.8504  0.9259   \n",
       "21  embeddings_microsoft_deberta_base_corr_2000  0.6823  0.8408  0.9224   \n",
       "22  embeddings_microsoft_deberta_base_corr_4000  0.5668  0.8293  0.9220   \n",
       "23  embeddings_microsoft_deberta_base_corr_6000  0.6209  0.8341  0.9228   \n",
       "24                  embeddings_xlnet_base_cased  0.7545  0.8035  0.9158   \n",
       "\n",
       "      mrpc    sst2     qqp  bigram_shift_layer_0  \\\n",
       "0   0.8995  0.9438  0.9143                   0.5   \n",
       "1   0.8750  0.9415  0.9164                   0.5   \n",
       "2   0.8824  0.9392  0.9160                   0.5   \n",
       "3   0.8824  0.9415  0.9166                   0.5   \n",
       "4   0.8873  0.9450  0.9165                   0.5   \n",
       "5   0.8554  0.9392  0.9169                   0.5   \n",
       "6   0.8211  0.9289  0.9093                   0.5   \n",
       "7   0.8897  0.9255  0.7834                   0.5   \n",
       "8   0.8284  0.9255  0.7875                   0.5   \n",
       "9   0.8088  0.9151  0.7821                   0.5   \n",
       "10  0.8456  0.9278  0.7846                   0.5   \n",
       "11  0.8015  0.9289  0.7875                   0.5   \n",
       "12  0.8775  0.9266  0.7759                   0.5   \n",
       "13  0.8480  0.9220  0.7708                   0.5   \n",
       "14  0.8284  0.9197  0.7757                   0.5   \n",
       "15  0.8113  0.9151  0.7581                   0.5   \n",
       "16  0.8113  0.9186  0.7755                   0.5   \n",
       "17  0.8088  0.9209  0.7722                   0.5   \n",
       "18  0.8946  0.9518  0.7865                   0.5   \n",
       "19  0.8897  0.9427  0.7742                   0.5   \n",
       "20  0.8456  0.9415  0.7822                   0.5   \n",
       "21  0.8897  0.9472  0.7831                   0.5   \n",
       "22  0.8603  0.9438  0.7874                   0.5   \n",
       "23  0.8701  0.9427  0.7868                   0.5   \n",
       "24  0.8676  0.9392  0.7805                   0.5   \n",
       "\n",
       "    coordination_inversion_layer_0  obj_number_layer_0  ...  \\\n",
       "0                              0.5                 0.5  ...   \n",
       "1                              0.5                 0.5  ...   \n",
       "2                              0.5                 0.5  ...   \n",
       "3                              0.5                 0.5  ...   \n",
       "4                              0.5                 0.5  ...   \n",
       "5                              0.5                 0.5  ...   \n",
       "6                              0.5                 0.5  ...   \n",
       "7                              0.5                 0.5  ...   \n",
       "8                              0.5                 0.5  ...   \n",
       "9                              0.5                 0.5  ...   \n",
       "10                             0.5                 0.5  ...   \n",
       "11                             0.5                 0.5  ...   \n",
       "12                             0.5                 0.5  ...   \n",
       "13                             0.5                 0.5  ...   \n",
       "14                             0.5                 0.5  ...   \n",
       "15                             0.5                 0.5  ...   \n",
       "16                             0.5                 0.5  ...   \n",
       "17                             0.5                 0.5  ...   \n",
       "18                             0.5                 0.5  ...   \n",
       "19                             0.5                 0.5  ...   \n",
       "20                             0.5                 0.5  ...   \n",
       "21                             0.5                 0.5  ...   \n",
       "22                             0.5                 0.5  ...   \n",
       "23                             0.5                 0.5  ...   \n",
       "24                             0.5                 0.5  ...   \n",
       "\n",
       "    past_present_layer_11  subj_number_layer_11  tree_depth_layer_11  \\\n",
       "0                0.857333              0.793333             0.283714   \n",
       "1                0.865333              0.704333             0.250381   \n",
       "2                0.819000              0.721333             0.259905   \n",
       "3                0.864333              0.747667             0.276190   \n",
       "4                0.864333              0.785667             0.284190   \n",
       "5                0.867667              0.803333             0.287619   \n",
       "6                0.757667              0.745667             0.267143   \n",
       "7                0.773333              0.746667             0.260952   \n",
       "8                0.774000              0.783667             0.262762   \n",
       "9                0.793667              0.777333             0.260857   \n",
       "10               0.805333              0.775667             0.274381   \n",
       "11               0.810333              0.784667             0.269143   \n",
       "12               0.882000              0.799667             0.301905   \n",
       "13               0.852667              0.814000             0.295429   \n",
       "14               0.856333              0.796333             0.287048   \n",
       "15               0.835667              0.788000             0.299524   \n",
       "16               0.850333              0.798000             0.298667   \n",
       "17               0.850333              0.801667             0.294952   \n",
       "18               0.866000              0.804667             0.285714   \n",
       "19               0.865000              0.803000             0.293143   \n",
       "20               0.862333              0.780667             0.291714   \n",
       "21               0.865667              0.786667             0.294286   \n",
       "22               0.853333              0.781333             0.302000   \n",
       "23               0.862333              0.798000             0.297810   \n",
       "24               0.779000              0.737000             0.287905   \n",
       "\n",
       "    bigram_shift_layer_12  coordination_inversion_layer_12  \\\n",
       "0                0.850667                         0.654333   \n",
       "1                0.685667                         0.546667   \n",
       "2                0.605667                         0.547000   \n",
       "3                0.579000                         0.539333   \n",
       "4                0.559333                         0.540667   \n",
       "5                0.552333                         0.535667   \n",
       "6                0.763333                         0.565333   \n",
       "7                0.605333                         0.504000   \n",
       "8                0.558333                         0.517333   \n",
       "9                0.534000                         0.495667   \n",
       "10               0.531000                         0.509333   \n",
       "11               0.504667                         0.499667   \n",
       "12               0.821333                         0.679000   \n",
       "13               0.614667                         0.546333   \n",
       "14               0.583667                         0.548333   \n",
       "15               0.561333                         0.523333   \n",
       "16               0.541000                         0.512333   \n",
       "17               0.550333                         0.501333   \n",
       "18               0.773667                         0.631000   \n",
       "19               0.634333                         0.571667   \n",
       "20               0.608667                         0.557000   \n",
       "21               0.623333                         0.563333   \n",
       "22               0.604667                         0.566333   \n",
       "23               0.610333                         0.586000   \n",
       "24               0.621667                         0.583333   \n",
       "\n",
       "    obj_number_layer_12  odd_man_out_layer_12  past_present_layer_12  \\\n",
       "0              0.787333              0.619667               0.879667   \n",
       "1              0.813000              0.582667               0.868000   \n",
       "2              0.810000              0.564333               0.864333   \n",
       "3              0.810000              0.560667               0.870333   \n",
       "4              0.804333              0.570667               0.865333   \n",
       "5              0.813333              0.571333               0.865333   \n",
       "6              0.819000              0.558667               0.854667   \n",
       "7              0.794000              0.522000               0.826667   \n",
       "8              0.804667              0.525333               0.837333   \n",
       "9              0.813000              0.507333               0.844000   \n",
       "10             0.796000              0.514333               0.843000   \n",
       "11             0.807667              0.518667               0.849000   \n",
       "12             0.765667              0.577333               0.880667   \n",
       "13             0.803000              0.531000               0.834333   \n",
       "14             0.785333              0.519667               0.840000   \n",
       "15             0.776000              0.515667               0.839333   \n",
       "16             0.801667              0.528000               0.838000   \n",
       "17             0.785000              0.526333               0.839667   \n",
       "18             0.637667              0.558000               0.869333   \n",
       "19             0.558333              0.515000               0.835667   \n",
       "20             0.584000              0.507000               0.850000   \n",
       "21             0.600333              0.507667               0.861667   \n",
       "22             0.617000              0.506000               0.853333   \n",
       "23             0.670333              0.524333               0.859667   \n",
       "24             0.654667              0.542333               0.789667   \n",
       "\n",
       "    subj_number_layer_12  tree_depth_layer_12  \n",
       "0               0.815333             0.295238  \n",
       "1               0.806000             0.294857  \n",
       "2               0.814667             0.297714  \n",
       "3               0.812000             0.301143  \n",
       "4               0.810333             0.302667  \n",
       "5               0.810000             0.297810  \n",
       "6               0.817333             0.309429  \n",
       "7               0.800000             0.284952  \n",
       "8               0.803667             0.293143  \n",
       "9               0.811000             0.282571  \n",
       "10              0.809000             0.295048  \n",
       "11              0.810333             0.291143  \n",
       "12              0.794667             0.298190  \n",
       "13              0.813667             0.306762  \n",
       "14              0.798667             0.302762  \n",
       "15              0.778000             0.312000  \n",
       "16              0.793000             0.309048  \n",
       "17              0.794333             0.309429  \n",
       "18              0.745667             0.262857  \n",
       "19              0.672333             0.251524  \n",
       "20              0.706667             0.260571  \n",
       "21              0.721667             0.272762  \n",
       "22              0.747667             0.282381  \n",
       "23              0.790000             0.285810  \n",
       "24              0.740000             0.274286  \n",
       "\n",
       "[25 rows x 98 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile a dataframe for linear regression\n",
    "def prepare_data(probing_df, glue_df):\n",
    "\n",
    "    lms = glue_df[\"LM\"].drop_duplicates().tolist()\n",
    "    result_df = {\"LM\": lms}\n",
    "    for lm in lms:\n",
    "        # Classification\n",
    "        # Each task: one LM (one row in glue_df) as target. \n",
    "        df = glue_df[glue_df.LM==lm]  # n_classification_task rows\n",
    "        for i, row in df.iterrows():\n",
    "            if row.task not in result_df:\n",
    "                result_df[row.task] = [row.dev_acc]\n",
    "            else:\n",
    "                result_df[row.task].append(row.dev_acc)\n",
    "\n",
    "        # Probing\n",
    "        # 13 layers x 7 tasks = 91 (rows) as data from probing_df as features.\n",
    "        df = probing_df[probing_df.LM==lm]  # 91 rows  \n",
    "        for i, row in df.iterrows():\n",
    "            featname = \"{}_layer_{}\".format(row.task, row.layer)\n",
    "            featval = row.test_acc \n",
    "            if featname in result_df:\n",
    "                result_df[featname].append(featval)\n",
    "            else:\n",
    "                result_df[featname] = [featval]\n",
    "    \n",
    "    return pd.DataFrame(result_df)\n",
    "\n",
    "linreg_data = prepare_data(probing_df, glue_df)\n",
    "linreg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_data.to_csv(\"../reports/task1_predict_task_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>rte</td>       <th>  R-squared:         </th> <td>   0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Jan 2022</td> <th>  Prob (F-statistic):</th>  <td>0.0918</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:11:16</td>     <th>  Log-Likelihood:    </th> <td>  48.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    25</td>      <th>  AIC:               </th> <td>  -70.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    12</td>      <th>  BIC:               </th> <td>  -54.34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_0</th>  <td>    3.0911</td> <td>    2.831</td> <td>    1.092</td> <td> 0.296</td> <td>   -3.078</td> <td>    9.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_1</th>  <td>   -2.2583</td> <td>    2.711</td> <td>   -0.833</td> <td> 0.421</td> <td>   -8.164</td> <td>    3.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_2</th>  <td>   -0.1363</td> <td>    0.962</td> <td>   -0.142</td> <td> 0.890</td> <td>   -2.232</td> <td>    1.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_3</th>  <td>   -0.1716</td> <td>    1.278</td> <td>   -0.134</td> <td> 0.895</td> <td>   -2.956</td> <td>    2.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_4</th>  <td>   -0.3510</td> <td>    0.998</td> <td>   -0.352</td> <td> 0.731</td> <td>   -2.525</td> <td>    1.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_5</th>  <td>    1.7382</td> <td>    1.244</td> <td>    1.397</td> <td> 0.188</td> <td>   -0.973</td> <td>    4.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_6</th>  <td>   -1.0122</td> <td>    1.546</td> <td>   -0.655</td> <td> 0.525</td> <td>   -4.381</td> <td>    2.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_7</th>  <td>   -0.1937</td> <td>    2.354</td> <td>   -0.082</td> <td> 0.936</td> <td>   -5.323</td> <td>    4.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_8</th>  <td>    0.6034</td> <td>    2.019</td> <td>    0.299</td> <td> 0.770</td> <td>   -3.796</td> <td>    5.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_9</th>  <td>   -0.8091</td> <td>    1.106</td> <td>   -0.732</td> <td> 0.478</td> <td>   -3.219</td> <td>    1.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_10</th> <td>    0.6890</td> <td>    1.415</td> <td>    0.487</td> <td> 0.635</td> <td>   -2.394</td> <td>    3.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_11</th> <td>    0.0685</td> <td>    1.080</td> <td>    0.063</td> <td> 0.951</td> <td>   -2.285</td> <td>    2.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bigram_shift_layer_12</th> <td>   -0.0789</td> <td>    0.984</td> <td>   -0.080</td> <td> 0.937</td> <td>   -2.224</td> <td>    2.066</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.927</td> <th>  Durbin-Watson:     </th> <td>   2.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.629</td> <th>  Jarque-Bera (JB):  </th> <td>   0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.297</td> <th>  Prob(JB):          </th> <td>   0.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.070</td> <th>  Cond. No.          </th> <td>    889.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    rte   R-squared:                       0.689\n",
       "Model:                            OLS   Adj. R-squared:                  0.377\n",
       "Method:                 Least Squares   F-statistic:                     2.211\n",
       "Date:                Sat, 22 Jan 2022   Prob (F-statistic):             0.0918\n",
       "Time:                        21:11:16   Log-Likelihood:                 48.090\n",
       "No. Observations:                  25   AIC:                            -70.18\n",
       "Df Residuals:                      12   BIC:                            -54.34\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "bigram_shift_layer_0      3.0911      2.831      1.092      0.296      -3.078       9.260\n",
       "bigram_shift_layer_1     -2.2583      2.711     -0.833      0.421      -8.164       3.648\n",
       "bigram_shift_layer_2     -0.1363      0.962     -0.142      0.890      -2.232       1.960\n",
       "bigram_shift_layer_3     -0.1716      1.278     -0.134      0.895      -2.956       2.613\n",
       "bigram_shift_layer_4     -0.3510      0.998     -0.352      0.731      -2.525       1.823\n",
       "bigram_shift_layer_5      1.7382      1.244      1.397      0.188      -0.973       4.449\n",
       "bigram_shift_layer_6     -1.0122      1.546     -0.655      0.525      -4.381       2.357\n",
       "bigram_shift_layer_7     -0.1937      2.354     -0.082      0.936      -5.323       4.935\n",
       "bigram_shift_layer_8      0.6034      2.019      0.299      0.770      -3.796       5.003\n",
       "bigram_shift_layer_9     -0.8091      1.106     -0.732      0.478      -3.219       1.600\n",
       "bigram_shift_layer_10     0.6890      1.415      0.487      0.635      -2.394       3.772\n",
       "bigram_shift_layer_11     0.0685      1.080      0.063      0.951      -2.285       2.422\n",
       "bigram_shift_layer_12    -0.0789      0.984     -0.080      0.937      -2.224       2.066\n",
       "==============================================================================\n",
       "Omnibus:                        0.927   Durbin-Watson:                   2.338\n",
       "Prob(Omnibus):                  0.629   Jarque-Bera (JB):                0.374\n",
       "Skew:                           0.297   Prob(JB):                        0.830\n",
       "Kurtosis:                       3.070   Cond. No.                         889.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = linreg_data[\"rte\"]\n",
    "#X = linreg_data.drop(columns=[\"LM\", \"rte\", \"cola\", \"qnli\", \"mrpc\", \"sst2\", \"qqp\"])\n",
    "X = linreg_data[[f\"bigram_shift_layer_{i}\" for i in range(13)]]\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bigram_shift_layer_0     0.296370\n",
       "bigram_shift_layer_1     0.421051\n",
       "bigram_shift_layer_2     0.889690\n",
       "bigram_shift_layer_3     0.895450\n",
       "bigram_shift_layer_4     0.731088\n",
       "bigram_shift_layer_5     0.187712\n",
       "bigram_shift_layer_6     0.525022\n",
       "bigram_shift_layer_7     0.935777\n",
       "bigram_shift_layer_8     0.770182\n",
       "bigram_shift_layer_9     0.478428\n",
       "bigram_shift_layer_10    0.635115\n",
       "bigram_shift_layer_11    0.950519\n",
       "bigram_shift_layer_12    0.937407\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91113a6d945a23a1d7d4073c51114eab5e8effc568ceeb9bfdc399b075c747f0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('pytorch12': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
